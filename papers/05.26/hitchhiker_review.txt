Reviewer's Name: 
Jeff McGovern

Paper Title:
A hitchhiker's guide to fast and efficient data reconstruction in erasure-coded data centers

Provide a short summary of the paper:
Building on top of Reed-Solomon codes, Facebook engineers build a method of encoding that requires less data to be transferred over the network in reconstruction and lay the data onto the hard drive in such a way that read performance is maximized.

What are the strengths of the paper? (1-3 sentences):
The technique is built upon solid mathematical foundations and is a clever way to mitigate network affects.

What are the weaknesses of the paper? (1-3 sentences):
The results are okay (only around 30% improvement) and may not be enticing enough for other developers to take hold of it. They provide a poor overview of RS coding and their piggybacking method.

Overall paper merit (1 [worst] - 10 [best] ):
7

Provide detailed comments (expand on strengths/weaknesses; >= 2 paragraphs):
This technique of data reconstruction is incredibly interesting. The optimizations that the engineers have designed are solidly rooted and make sense. The gains achieved appear to be reasonably significant, in the context of information theory.

They show measurable improvements, but it may not be enough for other to adopt. The infrastructure needed to make this technique viable also isn't clear. There appears to be a lot of math making this happen --- how fast do the processors need to be to make this a viable option?

The background explanations of RS coding and piggybacking were mostly useless. I had to read the original papers on it, and then read the subsequent section and it made sense. It might be that the following section is where the _real_ explanation is and I jumped the gun on trying to understand it.
